{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ESP Experiment\n",
        "\n",
        "This notebook defines an agent to play the ESP card game. It runs on Python 3.8."
      ],
      "metadata": {
        "id": "IsmSJZ-4mu9X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyQTXlXAFzqR"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "from typing import Optional\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import os, sys, pdb, pickle\n",
        "from multiprocessing import Pool\n",
        "import time, math\n",
        "from copy import deepcopy\n",
        "\n",
        "import abc\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import plotnine as gg\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "\n",
        "colors = {0: 'b',\n",
        "          1: 'g',\n",
        "          2: 'r',\n",
        "          3: 'c',\n",
        "          4: 'm',\n",
        "          5: 'y',\n",
        "          6: 'k'}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interfaces, utilities and experiment constants"
      ],
      "metadata": {
        "id": "8fSFpF2jF920"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Agent interface\n",
        "class Agent(metaclass=abc.ABCMeta):\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def reset(self, rng: np.random.Generator, num_action: int) -> None:\n",
        "    # Reset the agent.\n",
        "    raise NotImplementedError\n",
        " \n",
        "  @abc.abstractmethod\n",
        "  def update(self, action: int, obs: np.ndarray) -> None:\n",
        "    # Update agent state.\n",
        "    raise NotImplementedError\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def select_action(self) -> int:\n",
        "    # Select an action.\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "Z-1sr-O2F8ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Experiment constants\n",
        "TYPES = 3\n",
        "NUM_CARDS = 3\n",
        "initial = (3, 3, 3)"
      ],
      "metadata": {
        "id": "Cs6LgaVZHou8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utilities for running the experiments\n",
        "def random_argmax(rng, scores: np.ndarray):\n",
        "  probs = (scores==scores.max()).astype(np.float32)\n",
        "  probs /= probs.sum()\n",
        "  return rng.choice(np.arange(scores.size), p=probs)\n",
        "\n",
        "def run_ESP_experiment(\n",
        "    num_sims: int, \n",
        "    agent,\n",
        "    types_of_cards: int = TYPES,\n",
        "    num_of_each_type: int = NUM_CARDS,\n",
        "    plot_results=False):\n",
        "  \n",
        "  num_action = types_of_cards\n",
        "  sims = range(num_sims)\n",
        "  num_timesteps = types_of_cards * num_of_each_type\n",
        "  timesteps = range(num_timesteps)\n",
        "  actions = list(range(num_action))\n",
        "  rewards = []\n",
        "\n",
        "  action_count = []\n",
        "\n",
        "  for sim in sims:\n",
        "    rng = np.random.default_rng(sim)\n",
        "    agent.reset(rng)\n",
        "    true = [i for i in range(TYPES) for j in range(NUM_CARDS)]\n",
        "    rng.shuffle(true)\n",
        "    # TODO: Change regret calculation method\n",
        "    action_count += [[num_timesteps*[0] for a in actions]]\n",
        "    rewards += [num_timesteps * [0]]\n",
        "    for timestep in timesteps:\n",
        "      a = agent.select_action()\n",
        "      obs = (true[timestep] == a)\n",
        "      agent.update(a, obs)\n",
        "\n",
        "      action_count[sim][a][timestep] = 1\n",
        "      rewards[sim][timestep] += obs\n",
        "\n",
        "    action_count1 = [[float(action_count[0][a][t]) \n",
        "    for t in range(num_timesteps)] for a in actions]\n",
        "    rewards1 = sum(rewards[0])\n",
        "  action_count = [[sum([float(action_count[sim][a][t]) \n",
        "    for sim in sims]) / num_sims \n",
        "    for t in range(num_timesteps)] for a in actions]\n",
        "  expected_reward = sum(sum(rewards,[])) / num_sims\n",
        "\n",
        "  if plot_results:\n",
        "    # plot action frequencies averaged over simulations\n",
        "    for a in actions:\n",
        "      plt.plot(timesteps,\n",
        "              pd.Series(action_count[a]).rolling(10, min_periods=1).mean(),\n",
        "              colors[a], label='$a =$' + str(a+1))\n",
        "    plt.axis([0,num_timesteps+1,0.0,1.01])\n",
        "    plt.xlabel(r'time $t$', fontsize=20)\n",
        "    plt.ylabel('$\\mathbb{P}(A_t = a|\\mathcal{E})$', fontsize=20)\n",
        "    pylab.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    # plot action counts over single simulation\n",
        "    for a in actions:\n",
        "      plt.plot(timesteps, np.cumsum(action_count1[a]), colors[a], label='$a =$' + str(a+1))\n",
        "    plt.axis([0,num_timesteps+1,0.0,num_timesteps+1])\n",
        "    plt.xlabel(r'time $t$', fontsize=20)\n",
        "    plt.ylabel('$N_{t,a}$', fontsize=20)\n",
        "    pylab.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    # plot action counts averaged over simulations\n",
        "    for a in actions:\n",
        "      plt.plot(timesteps, np.cumsum(action_count[a]), colors[a], label='$a =$' + str(a+1))\n",
        "    plt.axis([0,num_timesteps+1,0.0,num_timesteps+1])\n",
        "    plt.xlabel(r'time $t$', fontsize=20)\n",
        "    plt.ylabel('$\\mathrm{E}[N_{t,a}|\\mathcal{E}]$', fontsize=20)\n",
        "    pylab.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "  return action_count, expected_reward"
      ],
      "metadata": {
        "id": "-OsQBSWYIjyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents"
      ],
      "metadata": {
        "id": "23z7JRCxbL8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title An agent that learns from observation\n",
        "class LearnFromObsAgent(Agent):\n",
        "  def __init__(self, compute_action, initial: tuple):\n",
        "    self._compute_action = compute_action\n",
        "    self._initial = initial\n",
        "    self._states = {self._initial: 1}\n",
        "    self._num_type  = len(initial)\n",
        "    self._type_prob = np.ones(self._num_type) / self._num_type\n",
        "    self._rng = None\n",
        "\n",
        "  def reset(self, rng: np.random.Generator):\n",
        "    self._rng = rng\n",
        "    self._states = {self._initial: 1}\n",
        "    self._type_prob = np.ones(self._num_type) / self._num_type\n",
        "    \n",
        "  def update(self, action: int, correct: bool):\n",
        "    next_states = {}\n",
        "    for comp, prob in self._states.items():\n",
        "      if correct and comp[action]:\n",
        "        posterior = (comp[action] / sum(comp)) * prob / self._type_prob[action]\n",
        "        cl = list(comp)\n",
        "        cl[action] -= 1\n",
        "        cl = tuple(cl)\n",
        "        if cl not in next_states: next_states[cl] = 0\n",
        "        next_states[cl] += posterior\n",
        "      if not correct and sum(comp) - comp[action]:\n",
        "        posterior_0 = (1 - (comp[action] / sum(comp))) * prob / (1 - self._type_prob[action])\n",
        "        for other in range(self._num_type):\n",
        "          if other == action or not comp[other]: continue\n",
        "          posterior = posterior_0 * comp[other] / (sum(comp) - comp[action])\n",
        "          cl = list(comp)\n",
        "          cl[other] -= 1\n",
        "          cl = tuple(cl)\n",
        "          if cl not in next_states: next_states[cl] = 0\n",
        "          next_states[cl] += posterior \n",
        "    self._states = next_states\n",
        "    # update type probs: there may be a faster incremental way\n",
        "    self._type_prob = np.zeros(self._num_type)\n",
        "    for comp, prob in self._states.items():\n",
        "      ca = np.array(comp)\n",
        "      if ca.sum() != 0:\n",
        "        self._type_prob += ca / ca.sum() * prob\n",
        "\n",
        "  def select_action(self):\n",
        "    return self._compute_action(self._rng, self._type_prob)"
      ],
      "metadata": {
        "id": "Yly23CqubOBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Greedy Action\n",
        "def greedy_action(rng, type_prob):\n",
        "  return random_argmax(rng, type_prob)"
      ],
      "metadata": {
        "id": "-Jc7OQZOcLrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title TS Action\n",
        "def ts_action(rng, type_prob):\n",
        "  num_type = len(type_prob)\n",
        "  return rng.choice(range(num_type), p=type_prob)"
      ],
      "metadata": {
        "id": "rT5N0cHDcSEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title IDS Action\n",
        "def ids_action(rng, type_prob):\n",
        "  num_type = len(type_prob)\n",
        "  mutual_infos = [scipy.stats.entropy([1-p, p]) for p in type_prob]\n",
        "  shortfall = 1 - type_prob\n",
        "  minimum = float('inf')\n",
        "  min_actions = [-1, -1]\n",
        "  min_alpha = -1\n",
        "  for a in range(num_type):\n",
        "    func = (shortfall[a])**2 / (mutual_infos[a])\n",
        "    if func < minimum:\n",
        "      minimum, min_actions, min_alpha = func, [a, 0], 1\n",
        "    for b in range(a, num_type):\n",
        "      func = lambda x: (x * shortfall[a] + (1 - x) * shortfall[b])**2 / (x * mutual_infos[a] + (1 - x) * mutual_infos[b])\n",
        "      res = scipy.optimize.minimize(func, 0, bounds=[(0, 1)])\n",
        "      if res.fun < minimum:\n",
        "        minimum, min_actions, min_alpha = res.fun, [a, b], res.x[0]\n",
        "  if min_alpha == -1:\n",
        "    return random_argmax(rng, type_prob)\n",
        "  return rng.choice(min_actions, p=[min_alpha, 1 - min_alpha])\n"
      ],
      "metadata": {
        "id": "RAb40XNgcPDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test evaluation\n",
        "num_sims = 1000 # number of simulations over which to average"
      ],
      "metadata": {
        "id": "RXVJxlXFjpaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_agent = LearnFromObsAgent(greedy_action, initial)\n",
        "\n",
        "action_count_greedy, reward_greedy = run_ESP_experiment(\n",
        "    num_sims,\n",
        "    greedy_agent)\n",
        "\n",
        "reward_greedy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InvLkLlS56sO",
        "outputId": "5e3b55b8-8dda-4f02-db5b-b95dca698537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.221"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ts_agent = LearnFromObsAgent(ts_action, initial)\n",
        "\n",
        "action_count_ts, reward_ts = run_ESP_experiment(\n",
        "    num_sims,\n",
        "    ts_agent)\n",
        "reward_ts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXEaUD3KMxFC",
        "outputId": "3a2060a8-5e72-4e97-f4cb-901f4d6e356d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.426"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_agent = LearnFromObsAgent(ids_action, initial)\n",
        "\n",
        "action_count_ids, reward_ids = run_ESP_experiment(\n",
        "    num_sims,\n",
        "    ids_agent)\n",
        "reward_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv95CS_MfZK9",
        "outputId": "26a7f2ac-d7f9-4dd0-b170-41fa3f5dd362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-76-a8043ab9b687>:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "<ipython-input-76-a8043ab9b687>:14: RuntimeWarning: invalid value encountered in true_divide\n",
            "<ipython-input-76-a8043ab9b687>:14: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_numdiff.py:557: RuntimeWarning: invalid value encountered in subtract\n",
            "<ipython-input-76-a8043ab9b687>:10: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "<ipython-input-76-a8043ab9b687>:14: RuntimeWarning: invalid value encountered in multiply\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.273"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Expectation calculation\n"
      ],
      "metadata": {
        "id": "c-kG7VjLblHq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}