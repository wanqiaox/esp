{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsmSJZ-4mu9X"
      },
      "source": [
        "# ESP Experiment\n",
        "\n",
        "This notebook defines an agent to play the ESP card game. It runs on Python 3.8.\n",
        "\n",
        "### Contents:\n",
        "1. Imports\n",
        "1. Interfaces, utilities and experiment constants\n",
        "    1. Agent interface\n",
        "    2. Experiment constants\n",
        "    3. Utilities for running the experiments\n",
        "2. Specific Agents\n",
        "    1. An agent that learns from observation\n",
        "    2. Greedy action\n",
        "    3. Optimal action\n",
        "    4. TS action\n",
        "    5. IDS action\n",
        "    6. Lagrangian-IDS action\n",
        "3. Sweeps\n",
        "    1. Information Agent vs $\\gamma$\n",
        "    2. Greedy, Optimal, Lagrangian-IDS action EV vs Initial Deck"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RyQTXlXAFzqR"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "from typing import Optional\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import os, sys, pdb, pickle\n",
        "from multiprocessing import Pool\n",
        "from fractions import Fraction\n",
        "import time, math\n",
        "from copy import deepcopy\n",
        "\n",
        "import abc\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "\n",
        "colors = {0: 'b',\n",
        "          1: 'g',\n",
        "          2: 'r',\n",
        "          3: 'c',\n",
        "          4: 'm',\n",
        "          5: 'y',\n",
        "          6: 'k'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fSFpF2jF920"
      },
      "source": [
        "# Interfaces, utilities and experiment constants\n",
        "Use this as the base class for agents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z-1sr-O2F8ue"
      },
      "outputs": [],
      "source": [
        "#@title Agent interface\n",
        "class Agent(metaclass=abc.ABCMeta):\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def reset(self, rng: np.random.Generator, num_action: int) -> None:\n",
        "    # Reset the agent.\n",
        "    raise NotImplementedError\n",
        " \n",
        "  @abc.abstractmethod\n",
        "  def update(self, action: int, obs: np.ndarray) -> None:\n",
        "    # Update agent state.\n",
        "    raise NotImplementedError\n",
        "\n",
        "  @abc.abstractmethod\n",
        "  def select_action(self) -> int:\n",
        "    # Select an action.\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cs6LgaVZHou8"
      },
      "outputs": [],
      "source": [
        "#@title Experiment constants\n",
        "TYPES = 3\n",
        "NUM_CARDS = 3\n",
        "initial = (3, 3, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-OsQBSWYIjyT"
      },
      "outputs": [],
      "source": [
        "#@title Utilities for running the experiments\n",
        "def random_argmax(rng, scores: np.ndarray):\n",
        "  probs = (scores==scores.max()).astype(np.float32)\n",
        "  probs /= probs.sum()\n",
        "  return rng.choice(np.arange(scores.size), p=probs)\n",
        "\n",
        "def generate_all_decks(initial):\n",
        "  '''\n",
        "  Given a compressed representation of the composition of cards, \n",
        "  generate a list of all decks\n",
        "  '''\n",
        "  all_decks = []\n",
        "  if sum(initial) == 1:\n",
        "    for i in range(len(initial)):\n",
        "      if initial[i] > 0: return [[i]]\n",
        "  for i in range(len(initial)):\n",
        "    if initial[i] > 0:\n",
        "      initial[i] -= 1\n",
        "      recur_decks = generate_all_decks(initial)\n",
        "      initial[i] += 1\n",
        "      all_decks += [[i] + deck for deck in recur_decks]\n",
        "  return all_decks\n",
        "\n",
        "all_decks = generate_all_decks([3,3,3])\n",
        "print('Number of Unique Decks:', len(all_decks))\n",
        "\n",
        "from functools import partial\n",
        "def f(l, a):\n",
        "  return a.run(l)\n",
        "\n",
        "def chunks(l, n):\n",
        "  for i in range(0, len(l), n):\n",
        "    yield l[i:i+n]\n",
        "\n",
        "def deck_ev(agent, initial, verbose=True, **kwargs):\n",
        "  #\n",
        "  nproc = 8\n",
        "  p = Pool(nproc)\n",
        "  batch_size = 2 * nproc\n",
        "  a = agent(**kwargs)\n",
        "  \n",
        "  all_decks = generate_all_decks(list(initial))\n",
        "  correct_guesses = []\n",
        "  for idx, decks in enumerate(chunks(all_decks, batch_size)):\n",
        "      correct_guesses += p.map(partial(f, a=a), decks)\n",
        "      if verbose:\n",
        "          print('\\rOn deck %s (%04d/%04d) | EV %.6f'%(''.join(map(str, decks[-1])), min((idx+1)*batch_size, len(all_decks)), len(all_decks), np.mean(correct_guesses)), end='')\n",
        "          sys.stdout.flush()\n",
        "  if verbose:\n",
        "      print()\n",
        "  p.terminate()\n",
        "  return np.mean(correct_guesses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Run randomly simulated experiments\n",
        "def run_ESP_experiment(\n",
        "    num_sims: int, \n",
        "    agent,\n",
        "    types_of_cards: int = TYPES,\n",
        "    num_of_each_type: int = NUM_CARDS,\n",
        "    plot_results=False):\n",
        "  '''\n",
        "  Evaluate agents based on random simulations\n",
        "  '''\n",
        "  num_action = types_of_cards\n",
        "  sims = range(num_sims)\n",
        "  num_timesteps = types_of_cards * num_of_each_type\n",
        "  timesteps = range(num_timesteps)\n",
        "  actions = list(range(num_action))\n",
        "  rewards = []\n",
        "\n",
        "  action_count = []\n",
        "\n",
        "  for sim in sims:\n",
        "    rng = np.random.default_rng(sim)\n",
        "    agent.reset(rng)\n",
        "    true = [i for i in range(TYPES) for j in range(NUM_CARDS)]\n",
        "    rng.shuffle(true)\n",
        "    # TODO: Change regret calculation method\n",
        "    action_count += [[num_timesteps*[0] for a in actions]]\n",
        "    rewards += [num_timesteps * [0]]\n",
        "    for timestep in timesteps:\n",
        "      a = agent.select_action()\n",
        "      obs = (true[timestep] == a)\n",
        "      agent.update(a, obs)\n",
        "\n",
        "      action_count[sim][a][timestep] = 1\n",
        "      rewards[sim][timestep] += obs\n",
        "\n",
        "    action_count1 = [[float(action_count[0][a][t]) \n",
        "    for t in range(num_timesteps)] for a in actions]\n",
        "    rewards1 = sum(rewards[0])\n",
        "  action_count = [[sum([float(action_count[sim][a][t]) \n",
        "    for sim in sims]) / num_sims \n",
        "    for t in range(num_timesteps)] for a in actions]\n",
        "  expected_reward = sum(sum(rewards,[])) / num_sims\n",
        "\n",
        "  if plot_results:\n",
        "    # plot action frequencies averaged over simulations\n",
        "    for a in actions:\n",
        "      plt.plot(timesteps,\n",
        "              pd.Series(action_count[a]).rolling(10, min_periods=1).mean(),\n",
        "              colors[a], label='$a =$' + str(a+1))\n",
        "    plt.axis([0,num_timesteps+1,0.0,1.01])\n",
        "    plt.xlabel(r'time $t$', fontsize=20)\n",
        "    plt.ylabel('$\\mathbb{P}(A_t = a|\\mathcal{E})$', fontsize=20)\n",
        "    pylab.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    # plot action counts over single simulation\n",
        "    for a in actions:\n",
        "      plt.plot(timesteps, np.cumsum(action_count1[a]), colors[a], label='$a =$' + str(a+1))\n",
        "    plt.axis([0,num_timesteps+1,0.0,num_timesteps+1])\n",
        "    plt.xlabel(r'time $t$', fontsize=20)\n",
        "    plt.ylabel('$N_{t,a}$', fontsize=20)\n",
        "    pylab.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    # plot action counts averaged over simulations\n",
        "    for a in actions:\n",
        "      plt.plot(timesteps, np.cumsum(action_count[a]), colors[a], label='$a =$' + str(a+1))\n",
        "    plt.axis([0,num_timesteps+1,0.0,num_timesteps+1])\n",
        "    plt.xlabel(r'time $t$', fontsize=20)\n",
        "    plt.ylabel('$\\mathrm{E}[N_{t,a}|\\mathcal{E}]$', fontsize=20)\n",
        "    pylab.legend(loc='best')\n",
        "    plt.show()\n",
        "\n",
        "  return action_count, expected_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23z7JRCxbL8q"
      },
      "source": [
        "# Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yly23CqubOBu"
      },
      "outputs": [],
      "source": [
        "#@title An agent that learns from observation\n",
        "class LearnFromObsAgent(Agent):\n",
        "  def __init__(self, compute_action, initial=None, rng=None):\n",
        "    self._compute_action = compute_action\n",
        "    if initial: self.reset(initial)\n",
        "    self._rng = rng\n",
        "\n",
        "  def reset(self, initial, rng=None):\n",
        "    self._initial = initial\n",
        "    self._num_type  = len(initial) # number of card types\n",
        "    self._total = sum(self.initial) # total number of cards\n",
        "    self._states = {self._initial: Fraction(1)}\n",
        "    self._correct = 0\n",
        "    self._type_prob = [ Fraction(x, sum(self._initial)) for x in self._initial]\n",
        "    self._rng = rng\n",
        "\n",
        "  def update_type_prob(self):\n",
        "    if self._total:\n",
        "      self._type_prob = [0] * self._num_type\n",
        "      for comp, prob in self._states.items():\n",
        "        for i in range(len(comp)):\n",
        "          self._type_prob[i] += prob * comp[i] / self._total\n",
        "      if sum(self._type_prob) != 1:\n",
        "        print('Warning: sum of group probabilities %f != 1.0'%sum(self.group_prob))\n",
        "    \n",
        "  def update(self, action: int, correct: bool):\n",
        "    # update state\n",
        "    self._correct += correct\n",
        "    self._total -= 1\n",
        "    next_states = {}\n",
        "    for comp, prob in self._states.items():\n",
        "      if correct and comp[action]:\n",
        "        posterior = (comp[action] / sum(comp)) * prob / self._type_prob[action]\n",
        "        cl = list(comp)\n",
        "        cl[action] -= 1\n",
        "        cl = tuple(cl)\n",
        "        if cl not in next_states: next_states[cl] = 0\n",
        "        next_states[cl] += posterior\n",
        "      if not correct and sum(comp) - comp[action]:\n",
        "        posterior_0 = (1 - (comp[action] / sum(comp))) * prob / (1 - self._type_prob[action])\n",
        "        for other in range(self._num_type):\n",
        "          if other == action or not comp[other]: continue\n",
        "          posterior = posterior_0 * comp[other] / (sum(comp) - comp[action])\n",
        "          cl = list(comp)\n",
        "          cl[other] -= 1\n",
        "          cl = tuple(cl)\n",
        "          if cl not in next_states: next_states[cl] = 0\n",
        "          next_states[cl] += posterior \n",
        "    self._states = next_states\n",
        "    if not len(self._states): return False\n",
        "    if sum(self._states.values()) != 1:\n",
        "      print('Warning: sum of state probabilities %f != 1.0'%sum(self.states.values()))\n",
        "    self.update_type_prob()\n",
        "\n",
        "  def select_action(self):\n",
        "    return self._compute_action(self._rng, self._type_prob)\n",
        "  \n",
        "  def set_composition(self, composition):\n",
        "    # \n",
        "    example_element = list(composition.keys())[0]\n",
        "    self._num_type = len(example_element)\n",
        "    self._total = sum(example_element)\n",
        "    self._states = composition\n",
        "    self._correct = 0\n",
        "    self.update_type_prob()\n",
        "\n",
        "  def serialize(self):\n",
        "    #\n",
        "    return 'x'.join(map(str, self._initial)) + ' ' + ' '.join(sorted(['|'.join(map(str, k)) + ':%s'%str(v) for k,v in self._states.items()]))\n",
        "  \n",
        "  def ev(self, memo=None):\n",
        "    # Evaluate agents via recursion\n",
        "    if self._total <= 0: return 0\n",
        "    if memo is None: memo = {}\n",
        "    serial = self.serialize()\n",
        "    if serial in memo: return memo[serial]\n",
        "    action = self.select_action()\n",
        "    \n",
        "    copy0 = deepcopy(self)\n",
        "    if copy0.update(action, False): ev0 = copy0.ev(memo)\n",
        "    else: ev0 = 0\n",
        "    copy1 = deepcopy(self)\n",
        "    if copy1.update(action,  True): ev1 = copy1.ev(memo) + 1\n",
        "    else: ev1 = 0\n",
        "    \n",
        "    ev = (1 - self._type_prob[action]) * ev0 + self._type_prob[action] * ev1\n",
        "    memo[serial] = ev\n",
        "    return ev\n",
        "\n",
        "  def run(self, deck):\n",
        "    #\n",
        "    self.reset(tuple(np.bincount(deck)))\n",
        "    for card in deck:\n",
        "        action = self.select_action()\n",
        "        self.update(action, action==card)\n",
        "    return self._correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Jc7OQZOcLrW"
      },
      "outputs": [],
      "source": [
        "#@title Greedy Action\n",
        "def greedy_action(rng, type_prob):\n",
        "  return random_argmax(rng, type_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT5N0cHDcSEU"
      },
      "outputs": [],
      "source": [
        "#@title TS Action\n",
        "def ts_action(rng, type_prob):\n",
        "  num_type = len(type_prob)\n",
        "  return rng.choice(range(num_type), p=type_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAb40XNgcPDG"
      },
      "outputs": [],
      "source": [
        "#@title IDS Action\n",
        "def ids_action(rng, type_prob):\n",
        "  num_type = len(type_prob)\n",
        "  mutual_infos = [scipy.stats.entropy([1-p, p]) for p in type_prob]\n",
        "  shortfall = 1 - type_prob\n",
        "  minimum = float('inf')\n",
        "  min_actions = [-1, -1]\n",
        "  min_alpha = -1\n",
        "  for a in range(num_type):\n",
        "    func = (shortfall[a])**2 / (mutual_infos[a])\n",
        "    if func < minimum:\n",
        "      minimum, min_actions, min_alpha = func, [a, 0], 1\n",
        "    for b in range(a, num_type):\n",
        "      func = lambda x: (x * shortfall[a] + (1 - x) * shortfall[b])**2 / (x * mutual_infos[a] + (1 - x) * mutual_infos[b])\n",
        "      res = scipy.optimize.minimize(func, 0, bounds=[(0, 1)])\n",
        "      if res.fun < minimum:\n",
        "        minimum, min_actions, min_alpha = res.fun, [a, b], res.x[0]\n",
        "  if min_alpha == -1:\n",
        "    return random_argmax(rng, type_prob)\n",
        "  return rng.choice(min_actions, p=[min_alpha, 1 - min_alpha])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXVJxlXFjpaH"
      },
      "outputs": [],
      "source": [
        "#@title Test evaluation\n",
        "num_sims = 1000 # number of simulations over which to average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InvLkLlS56sO",
        "outputId": "5e3b55b8-8dda-4f02-db5b-b95dca698537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.221"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "greedy_agent = LearnFromObsAgent(greedy_action, initial)\n",
        "\n",
        "action_count_greedy, reward_greedy = run_ESP_experiment(\n",
        "    num_sims,\n",
        "    greedy_agent)\n",
        "\n",
        "reward_greedy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXEaUD3KMxFC",
        "outputId": "3a2060a8-5e72-4e97-f4cb-901f4d6e356d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.426"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ts_agent = LearnFromObsAgent(ts_action, initial)\n",
        "\n",
        "action_count_ts, reward_ts = run_ESP_experiment(\n",
        "    num_sims,\n",
        "    ts_agent)\n",
        "reward_ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv95CS_MfZK9",
        "outputId": "26a7f2ac-d7f9-4dd0-b170-41fa3f5dd362"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-76-a8043ab9b687>:10: RuntimeWarning: invalid value encountered in double_scalars\n",
            "<ipython-input-76-a8043ab9b687>:14: RuntimeWarning: invalid value encountered in true_divide\n",
            "<ipython-input-76-a8043ab9b687>:14: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.8/dist-packages/scipy/optimize/_numdiff.py:557: RuntimeWarning: invalid value encountered in subtract\n",
            "<ipython-input-76-a8043ab9b687>:10: RuntimeWarning: divide by zero encountered in double_scalars\n",
            "<ipython-input-76-a8043ab9b687>:14: RuntimeWarning: invalid value encountered in multiply\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4.273"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids_agent = LearnFromObsAgent(ids_action, initial)\n",
        "\n",
        "action_count_ids, reward_ids = run_ESP_experiment(\n",
        "    num_sims,\n",
        "    ids_agent)\n",
        "reward_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-kG7VjLblHq"
      },
      "outputs": [],
      "source": [
        "#@title Expectation calculation\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.16 ('esp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "b88315a35a6ccb147c524760f911ed947e31e85c532d72a8368878b80c8fea8c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
